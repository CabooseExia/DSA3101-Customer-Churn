{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a58b5de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from flask import Flask, jsonify\n",
    "from flask_cors import CORS\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import xgboost as XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "31cb7fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'C': 1000, 'penalty': 'l1', 'solver': 'saga'}, {'C': 10, 'penalty': 'l2', 'solver': 'saga'}, {'C': 10, 'penalty': 'l1', 'solver': 'liblinear'}, {'C': 1, 'penalty': 'l1', 'solver': 'saga'}], [{'booster': 'gbtree', 'learning_rate': 0.1, 'n_estimators': 100, 'objective': 'binary:logistic', 'eval_metric': 'logloss', 'max_depth': 6, 'min_child_weight': 10, 'gamma': 0.1, 'subsample': 1.0, 'colsample_bytree': 0.8, 'lambda': None, 'alpha': None}, {'booster': 'gbtree', 'learning_rate': 0.1, 'n_estimators': 100, 'objective': 'binary:logistic', 'eval_metric': 'logloss', 'max_depth': 6, 'min_child_weight': 10, 'gamma': 0.1, 'subsample': 1.0, 'colsample_bytree': 0.8, 'lambda': None, 'alpha': None}, {'booster': 'gbtree', 'learning_rate': 0.1, 'n_estimators': 100, 'objective': 'binary:logistic', 'eval_metric': 'logloss', 'max_depth': 6, 'min_child_weight': 10, 'gamma': 0.1, 'subsample': 1.0, 'colsample_bytree': 0.8, 'lambda': None, 'alpha': None}, {'booster': 'gbtree', 'learning_rate': 0.1, 'n_estimators': 100, 'objective': 'binary:logistic', 'eval_metric': 'logloss', 'max_depth': 6, 'min_child_weight': 10, 'gamma': 0.1, 'subsample': 1.0, 'colsample_bytree': 0.8, 'lambda': None, 'alpha': None}], [{'n_estimators': 100, 'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2}, {'n_estimators': 100, 'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2}, {'n_estimators': 100, 'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2}, {'n_estimators': 100, 'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2}]]\n"
     ]
    }
   ],
   "source": [
    "# Read in the tuned hyperparameters\n",
    "try:\n",
    "    # Open the JSON file\n",
    "    with open('Models.json', 'r') as file:\n",
    "        # Load the models\n",
    "        models = json.load(file)\n",
    "    # File is successfully opened and loaded\n",
    "    print(models)\n",
    "except FileNotFoundError:\n",
    "    # Handle the FileNotFoundError\n",
    "    print(\"The file 'Models.json' does not exist or cannot be opened.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93cd118e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Gender', 'ChurnDate', 'FirstPersona', 'SecondPersona', 'ThirdPersona', 'CombinedPersonas', 'CurrLifecycle', 'SocialInfluencer']\n"
     ]
    }
   ],
   "source": [
    "# Read in the list of columns that were not used to train\n",
    "try:\n",
    "    # Open the JSON file\n",
    "    with open('FeatureDropped.json', 'r') as file:\n",
    "        # Load the models\n",
    "        features_dropped = json.load(file)\n",
    "    # File is successfully opened and loaded\n",
    "    print(features_dropped)\n",
    "except FileNotFoundError:\n",
    "    # Handle the FileNotFoundError\n",
    "    print(\"The file 'FeatureDropped.json' does not exist or cannot be opened.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57251247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:     Gender  Age  Tenure    Balance  NumOfProducts  EstimatedSalary   ChurnDate  \\\n",
      "0    Male   38      15  158584.28              1         76640.29         NaN   \n",
      "1  Female   36       3   94216.05              1        179128.69  2024-03-28   \n",
      "2  Female   35      11       0.00              2         19842.91         NaN   \n",
      "3  Female   26      11  145980.23              1        131804.86  2023-09-08   \n",
      "4    Male   34       1       0.00              2         10063.75         NaN   \n",
      "\n",
      "   TransactionFreq  TransactionAmt  ServiceSupportFrequency  ...  \\\n",
      "0        15.092344   145925.493884                        8  ...   \n",
      "1         9.192966    73541.740488                       35  ...   \n",
      "2        29.023334    68936.936250                        7  ...   \n",
      "3        29.562389    53097.327051                        3  ...   \n",
      "4        53.452905     4434.315367                        8  ...   \n",
      "\n",
      "   CurrLifecycle  Happiness Savings Savant Digital Dynamos Trustee Tribe  \\\n",
      "0         Active          0      28.508287       24.198895     47.292818   \n",
      "1    Reactivated          0      14.342629       40.398406     45.258964   \n",
      "2         Active          0      41.009464       24.290221     34.700315   \n",
      "3         Active          0      35.205365       38.977368     25.817267   \n",
      "4         Active          0      29.319781       36.591087     34.089132   \n",
      "\n",
      "      FirstPersona    SecondPersona  ThirdPersona SocialInfluencer  \\\n",
      "0    Trustee Tribe              NaN           NaN                0   \n",
      "1    Trustee Tribe  Digital Dynamos           NaN                0   \n",
      "2   Savings Savant    Trustee Tribe           NaN                1   \n",
      "3  Digital Dynamos   Savings Savant           NaN                0   \n",
      "4  Digital Dynamos    Trustee Tribe           NaN                0   \n",
      "\n",
      "             CombinedPersonas  \n",
      "0               Trustee Tribe  \n",
      "1             Premium Patrons  \n",
      "2  Cost-Conscious Careseekers  \n",
      "3           Frugal Innovators  \n",
      "4             Premium Patrons  \n",
      "\n",
      "[5 rows x 40 columns]\n",
      "\n",
      "\n",
      "Test:     Gender  Age  Tenure    Balance  NumOfProducts  EstimatedSalary   ChurnDate  \\\n",
      "0  Female   31      15  102075.88              1         79577.48         NaN   \n",
      "1    Male   26      11       0.00              2         38190.78         NaN   \n",
      "2    Male   39      13  115163.32              3         69052.63  2024-02-05   \n",
      "3    Male   55       3   70263.83              1         62347.71         NaN   \n",
      "4  Female   31      11  106596.29              1         91305.77         NaN   \n",
      "\n",
      "   TransactionFreq  TransactionAmt  ServiceSupportFrequency  ...  \\\n",
      "0        53.279141    15990.638613                        2  ...   \n",
      "1        37.225557   206296.922840                       23  ...   \n",
      "2        49.093555    15988.197207                        6  ...   \n",
      "3         7.022558    73541.673778                       37  ...   \n",
      "4        40.257960   105175.336643                       13  ...   \n",
      "\n",
      "   CurrLifecycle  Happiness Savings Savant Digital Dynamos Trustee Tribe  \\\n",
      "0    Reactivated          0      24.449878       41.320293     34.229829   \n",
      "1         Active          0      35.196458       40.011068     24.792474   \n",
      "2         Active          0      47.124304       35.064935     17.810761   \n",
      "3         Active          0      47.434819       20.941968     31.623213   \n",
      "4    Reactivated          0      21.946170       37.474120     40.579710   \n",
      "\n",
      "      FirstPersona    SecondPersona  ThirdPersona SocialInfluencer  \\\n",
      "0  Digital Dynamos    Trustee Tribe           NaN                0   \n",
      "1  Digital Dynamos   Savings Savant           NaN                0   \n",
      "2   Savings Savant              NaN           NaN                0   \n",
      "3   Savings Savant              NaN           NaN                1   \n",
      "4    Trustee Tribe  Digital Dynamos           NaN                1   \n",
      "\n",
      "    CombinedPersonas  \n",
      "0    Premium Patrons  \n",
      "1  Frugal Innovators  \n",
      "2     Savings Savant  \n",
      "3     Savings Savant  \n",
      "4    Premium Patrons  \n",
      "\n",
      "[5 rows x 40 columns]\n"
     ]
    }
   ],
   "source": [
    "# Read in the train and test data\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "input_features = [column for column in train_data.columns if column not in features_dropped]\n",
    "print(\"Train: \", train_data.head())\n",
    "print(\"\\n\")\n",
    "print(\"Test: \", test_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df2b363a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove this after then has added this into his code\n",
    "train_data['Active'] = (train_data['CurrLifecycle'] == 'Active').astype(int)\n",
    "train_data['Reactivated'] = (train_data['CurrLifecycle'] == 'Reactivated').astype(int)\n",
    "train_data['Dormant'] = (train_data['CurrLifecycle'] == 'Dormant').astype(int)\n",
    "train_data['Churned'] = (train_data['CurrLifecycle'] == 'Churned').astype(int)\n",
    "test_data['Active'] = (test_data['CurrLifecycle'] == 'Active').astype(int)\n",
    "test_data['Reactivated'] = (test_data['CurrLifecycle'] == 'Reactivated').astype(int)\n",
    "test_data['Dormant'] = (test_data['CurrLifecycle'] == 'Dormant').astype(int)\n",
    "test_data['Churned'] = (test_data['CurrLifecycle'] == 'Churned').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d874df50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables for logistic regression\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "for column in train_data.columns:\n",
    "    if train_data[column].dtype == 'object':\n",
    "        train_data[column] = label_encoder.fit_transform(train_data[column])\n",
    "        test_data[column] = label_encoder.fit_transform(test_data[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c70ce0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy data sets to do encoding and scaling for logistic regression\n",
    "train_lgr = train_data.copy()\n",
    "test_lgr = test_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee5cc5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale dataset for logistic regression\n",
    "scaler = StandardScaler()\n",
    "\n",
    "train_lgr_features = train_lgr[[column for column in train_lgr.columns if column not in ['Active', 'Reactivated', 'Dormant', 'Churned']]]\n",
    "train_lgr_labels = train_lgr[[column for column in train_lgr.columns if column in ['Active', 'Reactivated', 'Dormant', 'Churned']]]\n",
    "\n",
    "train_lgr_features = pd.DataFrame(scaler.fit_transform(train_lgr_features), columns = train_lgr_features.columns)\n",
    "train_lgr = pd.concat([train_lgr_features, train_lgr_labels], axis = 1)\n",
    "\n",
    "\n",
    "test_lgr_features = test_lgr[[column for column in test_lgr.columns if column not in ['Active', 'Reactivated', 'Dormant', 'Churned']]]\n",
    "test_lgr_labels = test_lgr[[column for column in test_lgr.columns if column in ['Active', 'Reactivated', 'Dormant', 'Churned']]]\n",
    "\n",
    "test_lgr_features = pd.DataFrame(scaler.fit_transform(test_lgr_features), columns = test_lgr_features.columns)\n",
    "test_lgr = pd.concat([test_lgr_features, test_lgr_labels], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e2d8ffbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the lgr, xgb, and rf parameters into\n",
    "# a list of 4 dictionaries with key value pairs of hyperparamters for Active, Reactivated, Dormant, and Churned Classification\n",
    "lgr = models[0]\n",
    "xgb = models[1]\n",
    "rf = models[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c388446",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_all_lgr = [train_lgr['Active'], train_lgr['Reactivated'], train_lgr['Dormant'], train_lgr['Churned']]\n",
    "y_test_all_lgr = [test_lgr['Active'], test_lgr['Reactivated'], test_lgr['Dormant'], test_lgr['Churned']]\n",
    "\n",
    "y_train_all = [train_data['Active'], train_data['Reactivated'], train_data['Dormant'], train_data['Churned']]\n",
    "y_test_all = [test_data['Active'], test_data['Reactivated'], test_data['Dormant'], test_data['Churned']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca440673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train_all_lgr[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc89efa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m4\u001b[39m):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Train the lgr models and store in the list\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     lgr_model \u001b[38;5;241m=\u001b[39m LogisticRegression(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlgr[i])\n\u001b[1;32m----> 7\u001b[0m     lgr_model\u001b[38;5;241m.\u001b[39mfit(train_lgr[input_features], y_train_all_lgr[i])\n\u001b[0;32m      8\u001b[0m     lgr_models_list\u001b[38;5;241m.\u001b[39mappend(lgr_model)\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Train the xgb models and store in the list\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#     dtrain = XGB.DMatrix(data=train_data[input_features], label=y_train_all[i], enable_categorical=True)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#     xgb_model = XGB.train(xgb[i], dtrain)\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1227\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1221\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m effective_n_jobs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1222\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1223\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_jobs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m > 1 does not have any effect when\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1224\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msolver\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is set to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mliblinear\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_jobs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1225\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m = \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(effective_n_jobs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs))\n\u001b[0;32m   1226\u001b[0m         )\n\u001b[1;32m-> 1227\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_ \u001b[38;5;241m=\u001b[39m _fit_liblinear(\n\u001b[0;32m   1228\u001b[0m         X,\n\u001b[0;32m   1229\u001b[0m         y,\n\u001b[0;32m   1230\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mC,\n\u001b[0;32m   1231\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_intercept,\n\u001b[0;32m   1232\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_scaling,\n\u001b[0;32m   1233\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_weight,\n\u001b[0;32m   1234\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpenalty,\n\u001b[0;32m   1235\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdual,\n\u001b[0;32m   1236\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m   1237\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter,\n\u001b[0;32m   1238\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtol,\n\u001b[0;32m   1239\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state,\n\u001b[0;32m   1240\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m   1241\u001b[0m     )\n\u001b[0;32m   1242\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m   1244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m solver \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msag\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaga\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1179\u001b[0m, in \u001b[0;36m_fit_liblinear\u001b[1;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[0;32m   1177\u001b[0m     classes_ \u001b[38;5;241m=\u001b[39m enc\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[0;32m   1178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(classes_) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m-> 1179\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1180\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis solver needs samples of at least 2 classes\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1181\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in the data, but the data contains only one\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1182\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m class: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1183\u001b[0m             \u001b[38;5;241m%\u001b[39m classes_[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1184\u001b[0m         )\n\u001b[0;32m   1186\u001b[0m     class_weight_ \u001b[38;5;241m=\u001b[39m compute_class_weight(class_weight, classes\u001b[38;5;241m=\u001b[39mclasses_, y\u001b[38;5;241m=\u001b[39my)\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0"
     ]
    }
   ],
   "source": [
    "lgr_models_list = []\n",
    "xgb_models_list = []\n",
    "rf_models_list = []\n",
    "for i in range(4):\n",
    "    # Train the lgr models and store in the list\n",
    "    lgr_model = LogisticRegression(**lgr[i])\n",
    "    lgr_model.fit(train_lgr[input_features], y_train_all_lgr[i])\n",
    "    lgr_models_list.append(lgr_model)\n",
    "    \n",
    "    # Train the xgb models and store in the list\n",
    "#     dtrain = XGB.DMatrix(data=train_data[input_features], label=y_train_all[i], enable_categorical=True)\n",
    "#     xgb_model = XGB.train(xgb[i], dtrain)\n",
    "    print(i)\n",
    "    xgb_model = XGB.XGBClassifier(**xgb[i])\n",
    "    xgb_model.fit(train_data[input_features], y_train_all[i])\n",
    "    xgb_models_list.append(xgb_model)\n",
    "    \n",
    "    # Train the rf models and store in the list\n",
    "    rf_model = RandomForestClassifier(**rf[i])\n",
    "    rf_model.fit(train_data[input_features], y_train_all[i])\n",
    "    rf_models_list.append(rf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb586f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to generate predictions from the trained models\n",
    "def prediction(lgr, xgb, rf, lgr_data, features): \n",
    "    # Generate predictions for data\n",
    "    for i in range(len(lgr)):\n",
    "        # lgr[0], xgb[0], rf[0] is the model with binary label Active/Non-Active\n",
    "        if i == 0:\n",
    "            data['lgr_Active_proba'] = lgr[0].predict_proba(features)\n",
    "            data['xgb_Active_proba'] = xgb[0].predict_proba(features)\n",
    "            data['rf_Active_proba'] = rf[0].predict_proba(features)\n",
    "            \n",
    "        # lgr[1], xgb[1], rf[1] is the model with binary label Reactivated/Non-Reactivated\n",
    "        elif i == 1:\n",
    "            data['lgr_Reactivated_proba'] = lgr[1].predict_proba(features)\n",
    "            data['xgb_Reactivated_proba'] = xgb[1].predict_proba(features)\n",
    "            data['rf_Reactivated_proba'] = rf[1].predict_proba(features)\n",
    "            \n",
    "        # lgr[2], xgb[2], rf[2] is the model with binary label Dormant/Non-Dormant\n",
    "        elif i == 2:\n",
    "            data['lgr_Dormant_proba'] = lgr[2].predict_proba(features)\n",
    "            data['xgb_Dormant_proba'] = xgb[2].predict_proba(features)\n",
    "            data['rf_Dormant_proba'] = rf[2].predict_proba(features)\n",
    "           \n",
    "        # lgr[3], xgb[3], rf[3] is the model with binary label Churned/Non-Churned\n",
    "        elif i == 3:\n",
    "            data['lgr_Churned_proba'] = lgr[3].predict_proba(features)\n",
    "            data['xgb_Churned_proba'] = xgb[3].predict_proba(features)\n",
    "            data['rf_Churned_proba'] = rf[3].predict_proba(features)\n",
    "    \n",
    "    # Calculate the average probability from the probabilities generated by each model (Ensemble Learning)\n",
    "    data['average_Active_proba'] = data[['lgr_Active_proba', 'xgb_Active_proba', 'rf_Active_proba']].agg(mean, axis = 1)\n",
    "    data['average_Reactivated_proba'] = data[['lgr_Reactivated_proba', 'xgb_Reactivated_proba', 'rf_Reactivated_proba']].agg(mean, axis = 1)\n",
    "    data['average_Dormant_proba'] = data[['lgr_Dormant_proba', 'xgb_Dormant_proba', 'rf_Dormant_proba']].agg(mean, axis = 1)\n",
    "    data['average_Churned_proba'] = data[['lgr_Churned_proba', 'xgb_Churned_proba', 'rf_Churned_proba']].agg(mean, axis = 1)\n",
    "    \n",
    "    # Based on the definition of lifecycle, it is not possible for a customer to have the below stated transitions\n",
    "    # Active -> Reactivated, Dormant -> Dormant, Dormant -> Active, Reactivated -> Reactivated\n",
    "    # Hence, set the probabilities of these cases to 0\n",
    "    data['average_Active_proba'] = np.where((data['Lifecycle'] == 'Dormant') & (data['average_Active_proba'] > 0), 0, data['average_Active_proba'])\n",
    "    data['average_Reactivated_proba'] = np.where((data['Lifecycle'] in ('Active', 'Reactivated')) & (data['average_Reactivated_proba'] > 0), 0, data['average_Reactivated_proba'])\n",
    "    data['average_Dormant_proba'] = np.where((data['Lifecycle'] == 'Dormant') & (data['average_Dormant_proba'] > 0), 0, data['average_Dormant_proba'])\n",
    "    \n",
    "    # The lifecycle with the highest probability will be the predicted lifecycle\n",
    "    max_proba = data[['average_Active_proba', 'average_Reactivated_proba', 'average_Dormant_proba', 'average_Churned_proba']].agg(max, axis = 1)\n",
    "    data['PredictedLifecycle'] = np.where(data['average_Active_proba'] == max_proba, 'Active', \\\n",
    "                                         +np.where(data['average_Reactivated_proba'] == max_proba, 'Reactivated', \\\n",
    "                                         +np.where(data['average_Dormant_proba'] == max_proba, 'Dormant', \\\n",
    "                                         +np.where(data['average_Churned_proba'] == max_proba, 'Churned'))))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d6b507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the existing active customers from the train_data\n",
    "train_drop_index = train_data[train_data['CurrLifecycle'] in ('Dormant', 'Churned')].index\n",
    "train_active = train_data.drop(train_drop_index)\n",
    "\n",
    "# Returns the df with predictions for train_active (This train data only consists of existing active customers)\n",
    "train_active_prediction = prediction(lgr_models_list, xgb_models_list, rf_models_list, train_active, input_features)\n",
    "\n",
    "# Returns the df with predictions for test_data \n",
    "# This test data consists of both active and churned customers to generate classification report for model performance evaluation\n",
    "test_prediction = prediction(gr_models_list, xgb_models_list, rf_models_list, test_data, input_features)\n",
    "\n",
    "# Generate classification report using the test predictions\n",
    "report = classification_report(test_prediction['CurrLifecycle'], test_prediction['PredictedLifecycle'])\n",
    "\n",
    "# Filter out the existing active customers from the test data\n",
    "test_drop_index = test_prediction[test_prediction['CurrLifecycle'] in ('Dormant', 'Churned')].index\n",
    "test_active_prediction = test_prediction.drop(test_drop_index)\n",
    "\n",
    "# Concat the train_active_prediction and test_active_prediction data\n",
    "predicted_data = pd.concat([train_active_prediction, test_active_prediction], ignore_index=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe21555c",
   "metadata": {},
   "outputs": [],
   "source": [
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11bf195",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60364b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flask API to send report and predicted data to frontend\n",
    "app = Flask(name)\n",
    "CORS(app)\n",
    "\n",
    "@app.route('/api/model')\n",
    "def get_model():\n",
    "    return jsonify(report)\n",
    "\n",
    "@app.route('/api/data')\n",
    "def get_data():\n",
    "    return jsonify(predicted_data.to_dict())\n",
    "\n",
    "if name == 'main':\n",
    "    app.run(debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
