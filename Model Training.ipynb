{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b45dc4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.utils import resample\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05dc1309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing of code, remove this afterwards, and add in the line to receive the processed data\n",
    "raw_train = pd.read_csv('train.csv')\n",
    "raw_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "990e79ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = raw_train.columns\n",
    "X = raw_train[cols[3:-1]].drop(columns=['Gender', 'Geography'])\n",
    "y = raw_train[\n",
    "[cols[-1]]]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54f5e95b-58e6-4e4b-9246-86c61b0f92a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_a, y_train_r, y_train_c, y_train_d = y_train, y_train, y_train, y_train\n",
    "y_test_a, y_test_r, y_test_c, y_test_d = y_test, y_test, y_test, y_test\n",
    "\n",
    "y_train_all = [y_train_a, y_train_r, y_train_c, y_train_d]\n",
    "y_test_all = [y_test_a, y_test_r, y_test_c, y_test_d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "374b0842-0a13-40ae-8290-2a6de236b21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to ignore the DataConversionWarning\n",
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "warnings.filterwarnings(\"ignore\", category=DataConversionWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d80892",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16c91119",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e391fc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning\n",
    "lr_param_dist = {\n",
    "    'penalty': ['l1', 'l2'],  \n",
    "    #'C' : np.logspace(-4, 4, 20),\n",
    "    'C': [0.1, 1, 10, 100, 1000],  \n",
    "    'solver': ['liblinear', 'saga'] \n",
    "}\n",
    "\n",
    "#search_lr = RandomizedSearchCV(lr_model, param_distributions=lr_param_dist, n_iter=10, scoring='f1_weighted', cv=5, random_state=42)\n",
    "\n",
    "# Scale the data for Logistic Regression\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Perform optimization\n",
    "#search_lr.fit(X_train_scaled, y_train)\n",
    "\n",
    "#print(\"Best parameters found: \", search_lr.best_params_)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "#best_model_lr = search_lr.best_estimator_\n",
    "#lr_predictions = best_model_lr.predict(X_test_scaled)\n",
    "#lr_probabilities = best_model_lr.predict_proba(X_test_scaled)[:,1]\n",
    "#test_score_lr = best_model_lr.score(X_test_scaled, y_test)\n",
    "#print(\"Test score of the best model: \", test_score_lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bff1402f-0a6e-4e3a-80a9-8cc37762efea",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_models = []\n",
    "lr_results = []\n",
    "\n",
    "for i in range(4):\n",
    "    lr_model = LogisticRegression()\n",
    "    search_lr = RandomizedSearchCV(lr_model, param_distributions=lr_param_dist, n_iter=10, scoring='f1_weighted', cv=5, random_state=42)\n",
    "\n",
    "    search_lr.fit(X_train_scaled, y_train_all[i])\n",
    "\n",
    "    best_model_lr = search_lr.best_estimator_\n",
    "    lr_models.append(best_model_lr)\n",
    "    \n",
    "    test_score_lr = best_model_lr.score(X_test_scaled, y_test_all[i])\n",
    "    lr_results.append(test_score_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f794669a-a35c-4b1f-bc8a-381d28ca907f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8233405035295543,\n",
       " 0.8233405035295543,\n",
       " 0.8233405035295543,\n",
       " 0.8233405035295543]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b265bd94",
   "metadata": {},
   "source": [
    "## Gradient Booster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f01d54-5718-4eb9-97eb-4201bf9bf374",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_model = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81638087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning\n",
    "gb_param_dist = {\n",
    "    'loss': ['log_loss', 'exponential'],\n",
    "    'learning_rate': np.arange(0., 2., 0.2).tolist(),\n",
    "    'n_estimators': np.arange(100, 301, 100).tolist(),\n",
    "    'criterion': ['friedman_mse', 'squared_error'],\n",
    "    'min_samples_leaf': np.arange(6,15, 2).tolist(),\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'min_samples_split': np.arange(2,15, 2).tolist(),\n",
    "    'max_depth': np.arange(3,16, 3).tolist()\n",
    "}\n",
    "\n",
    "#search = BayesSearchCV(\n",
    "#    estimator=gb_model,\n",
    "#    search_spaces=gb_param_dist,\n",
    "#    n_iter=50,\n",
    "#    cv=5\n",
    "#)\n",
    "\n",
    "search = RandomizedSearchCV(gb_model, param_distributions=gb_param_dist, n_iter=10, scoring='f1_weighted', cv=5, random_state=42)\n",
    "\n",
    "\n",
    "# Perform optimization\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters found: \", search.best_params_)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_model = search.best_estimator_\n",
    "gb_predictions = best_model.predict(X_test)\n",
    "test_score = best_model.score(X_test, y_test)\n",
    "print(\"Test score of the best model: \", test_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49c64141-910c-4b8d-aba0-4de956c26778",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "372aa2d6-d35b-4910-ad5b-bea4c623a2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'subsample': 1.0, 'objective': 'multi:softmax', 'num_class': 2, 'n_estimators': 100, 'min_child_weight': 10, 'max_depth': 6, 'learning_rate': 0.1, 'lambda': 2, 'gamma': 0.1, 'eval_metric': 'logloss', 'colsample_bytree': 0.8, 'booster': 'gbtree', 'alpha': 2}\n",
      "Test score of the best XGBoost model:  0.8563032084103372\n"
     ]
    }
   ],
   "source": [
    "# Define the XGBoost parameter ranges\n",
    "xgb_param_dist = {\n",
    "    'booster': ['gbtree'],  \n",
    "    'learning_rate': np.linspace(0.05, 0.3, 6),  \n",
    "    'n_estimators': [100, 200],  \n",
    "    'objective': ['multi:softmax'],  \n",
    "    'num_class': [2],  \n",
    "    'eval_metric': ['logloss'],  \n",
    "    'max_depth': [3, 6, 9],  \n",
    "    'min_child_weight': [1, 5, 10],  \n",
    "    'gamma': [0, 0.1, 0.2],  \n",
    "    'subsample': [0.6, 0.8, 1.0],  \n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],  \n",
    "    'lambda': [0, 1, 2],  \n",
    "    'alpha': [0, 1, 2]  \n",
    "}\n",
    "\n",
    "xgb_param_dist2 = {\n",
    "    'booster': ['gbtree', 'dart'],  \n",
    "    'learning_rate': np.linspace(0.05, 0.3, 6),  \n",
    "    'n_estimators': [100, 200, 300],  \n",
    "    'objective': ['binary:logistic', 'multi:softmax'],  \n",
    "    'num_class': [2],\n",
    "    'eval_metric': ['logloss'],  \n",
    "    'max_depth': [3, 6, 9, 12],  \n",
    "    'min_child_weight': [1, 5, 10],  \n",
    "    'gamma': [0, 0.1, 0.2],  \n",
    "    'subsample': [0.6, 0.8, 1.0],  \n",
    "    'colsample_bytree': [0.6, 0.8],  \n",
    "    'lambda': [0, 1, 2],  \n",
    "    'alpha': [0, 1, 2]  \n",
    "}\n",
    "\n",
    "# Instantiate RandomizedSearchCV\n",
    "#xgb_search = RandomizedSearchCV(xgb_model, param_distributions=xgb_param_dist, n_iter=10, scoring='f1_weighted', cv=5, random_state=42)\n",
    "\n",
    "# Perform optimization\n",
    "#xgb_search.fit(X_train, y_train)\n",
    "\n",
    "#print(\"Best parameters found: \", xgb_search.best_params_)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "#best_xgb_model = xgb_search.best_estimator_\n",
    "#xgb_predictions = best_xgb_model.predict(X_test)\n",
    "#test_score = best_xgb_model.score(X_test, y_test)\n",
    "#print(\"Test score of the best XGBoost model: \", test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b015ce-12e7-46ab-9193-72dfefa29f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_models = []\n",
    "xgb_results = []\n",
    "\n",
    "for i in range(4):\n",
    "    xgb_model = xgb.XGBClassifier()\n",
    "    xgb_search = RandomizedSearchCV(xgb_model, param_distributions=xgb_param_dist, n_iter=10, scoring='f1_weighted', cv=5, random_state=42)\n",
    "\n",
    "    xgb_search.fit(X_train, y_train_all[i])\n",
    "\n",
    "    best_model_xgb = xgb_search.best_estimator_\n",
    "    xgb_models.append(best_model_xgb)\n",
    "    \n",
    "    test_score_xgb = best_model_xgb.score(X_test, y_test_all[i])\n",
    "    xgb_results.append(test_score_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffac9f0",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c9ef173",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53d7f1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'n_estimators': 100, 'min_samples_split': 7, 'min_samples_leaf': 16, 'max_features': 'log2', 'max_depth': 15, 'criterion': 'entropy'}\n",
      "Test score of the best model:  0.8553943102978157\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter Tuning\n",
    "rf_param_dist = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None] + list(range(5, 16, 5)),\n",
    "    'min_samples_leaf': list(range(1, 21, 5)),\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'min_samples_split': list(range(2, 21, 5))\n",
    "}\n",
    "#search = BayesSearchCV(\n",
    "    #estimator=rf_model,\n",
    "    #search_spaces=rf_param_dist,\n",
    "    #n_iter=50,\n",
    "    #cv=5\n",
    "#)\n",
    "search = RandomizedSearchCV(rf_model, param_distributions=rf_param_dist, n_iter=10, scoring='f1_weighted', cv=5, random_state=42)\n",
    "\n",
    "# Perform optimization\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "#print(\"Best parameters found: \", search.best_params_)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "#best_model = search.best_estimator_\n",
    "#rf_predictions = best_model.predict(X_test)\n",
    "#test_score = best_model.score(X_test, y_test)\n",
    "#print(\"Test score of the best model: \", test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62d0e8c-1b23-4830-b824-c4d8db255d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_models = []\n",
    "rf_results = []\n",
    "\n",
    "for i in range(4):\n",
    "    rf_model = RandomForestClassifier()\n",
    "    search_rf = RandomizedSearchCV(rf_model, param_distributions=rf_param_dist, n_iter=10, scoring='f1_weighted', cv=5, random_state=42)\n",
    "\n",
    "    search_rf.fit(X_train, y_train_all[i])\n",
    "\n",
    "    best_model_rf = search_rf.best_estimator_\n",
    "    rf_models.append(best_model_rf)\n",
    "    \n",
    "    test_score_rf = best_model_rf.score(X_test, y_test_all[i])\n",
    "    rf_results.append(test_score_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a376ce-13be-4da1-b99d-d1ed39575f04",
   "metadata": {},
   "source": [
    "## Send Code to The Next Person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4feb8121-6513-480c-b6e4-eda41521c80a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
