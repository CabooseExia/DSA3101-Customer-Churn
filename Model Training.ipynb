{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b45dc4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.utils import resample\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05dc1309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing of code, remove this afterwards, and add in the line to receive the processed data\n",
    "raw_train = pd.read_csv('train.csv')\n",
    "raw_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "990e79ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = raw_train.columns\n",
    "X = raw_train[cols[3:-1]].drop(columns=['Gender', 'Geography'])\n",
    "y = raw_train[[cols[-1]]]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "374b0842-0a13-40ae-8290-2a6de236b21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to ignore the DataConversionWarning\n",
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "warnings.filterwarnings(\"ignore\", category=DataConversionWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d80892",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16c91119",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e391fc3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'solver': 'saga', 'penalty': 'l2', 'C': 100}\n",
      "Test score of the best model:  0.8233405035295543\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter Tuning\n",
    "lr_param_dist = {\n",
    "    'penalty': ['l1', 'l2'],  \n",
    "    #'C' : np.logspace(-4, 4, 20),\n",
    "    'C': [0.1, 1, 10, 100, 1000],  \n",
    "    'solver': ['liblinear', 'saga'] \n",
    "}\n",
    "\n",
    "#search = BayesSearchCV(\n",
    "    #estimator=lr_model,\n",
    "    #search_spaces=lr_param_dist,\n",
    "    #n_iter=50,\n",
    "    #cv=5\n",
    "#)\n",
    "search_lr = RandomizedSearchCV(lr_model, param_distributions=lr_param_dist, n_iter=10, scoring='f1_weighted', cv=5, random_state=42)\n",
    "# Scale the data for Logistic Regression\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Perform optimization\n",
    "search_lr.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Best parameters found: \", search_lr.best_params_)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_model_lr = search_lr.best_estimator_\n",
    "lr_predictions = best_model_lr.predict(X_test_scaled)\n",
    "lr_probabilities = best_model_lr.predict_proba(X_test_scaled)[:,1]\n",
    "test_score_lr = best_model_lr.score(X_test_scaled, y_test)\n",
    "print(\"Test score of the best model: \", test_score_lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b265bd94",
   "metadata": {},
   "source": [
    "## Gradient Booster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7a352cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_model = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81638087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning\n",
    "gb_param_dist = {\n",
    "    'loss': ['log_loss', 'exponential'],\n",
    "    'learning_rate': np.arange(0., 2., 0.2).tolist(),\n",
    "    'n_estimators': np.arange(100, 301, 100).tolist(),\n",
    "    'criterion': ['friedman_mse', 'squared_error'],\n",
    "    'min_samples_leaf': np.arange(6,15, 2).tolist(),\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'min_samples_split': np.arange(2,15, 2).tolist(),\n",
    "    'max_depth': np.arange(3,16, 3).tolist()\n",
    "}\n",
    "\n",
    "#search = BayesSearchCV(\n",
    "#    estimator=gb_model,\n",
    "#    search_spaces=gb_param_dist,\n",
    "#    n_iter=50,\n",
    "#    cv=5\n",
    "#)\n",
    "\n",
    "search = RandomizedSearchCV(gb_model, param_distributions=gb_param_dist, n_iter=10, scoring='f1_weighted', cv=5, random_state=42)\n",
    "\n",
    "\n",
    "# Perform optimization\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters found: \", search.best_params_)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_model = search.best_estimator_\n",
    "gb_predictions = best_model.predict(X_test)\n",
    "test_score = best_model.score(X_test, y_test)\n",
    "print(\"Test score of the best model: \", test_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffac9f0",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b174134d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4233ab76",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    # Initialize Random Forest classifier\n",
    "    rf_classifier = RandomForestClassifier(n_estimators=100)\n",
    "    \n",
    "    # Train the classifier for the i-th label\n",
    "    rf_classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Add the trained classifier to the list\n",
    "    classifiers.append(rf_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc736c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a66e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "for classifier in classifiers:\n",
    "    # Use predict_proba to get probability estimates\n",
    "    proba = classifier.predict_proba(X_test)\n",
    "    probabilities.append(proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9ef173",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d7f1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning\n",
    "rf_param_dist = {\n",
    "    'n_estimators': np.arange(100,201, 100).tolist(),\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "    'max_depth': np.arange(3,13, 3).tolist(),\n",
    "    'min_samples_leaf': np.arange(6,21).tolist()[0::2],\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'min_samples_split': np.arange(2,11, 2).tolist()\n",
    "}\n",
    "\n",
    "#search = BayesSearchCV(\n",
    "    #estimator=rf_model,\n",
    "    #search_spaces=rf_param_dist,\n",
    "    #n_iter=50,\n",
    "    #cv=5\n",
    "#)\n",
    "search = RandomizedSearchCV(rf_model, param_distributions=rf_param_dist, n_iter=10, scoring='f1_weighted', cv=5, random_state=42)\n",
    "\n",
    "# Perform optimization\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters found: \", search.best_params_)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_model = search.best_estimator_\n",
    "rf_predictions = best_model.predict(X_test)\n",
    "test_score = best_model.score(X_test, y_test)\n",
    "print(\"Test score of the best model: \", test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a376ce-13be-4da1-b99d-d1ed39575f04",
   "metadata": {},
   "source": [
    "## Ensemble & Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd365852-c3b3-4010-bb36-71fa29bd0230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Predictions: [0 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "logistic_regression_predictions = [0, 1, 1, 0, 1]\n",
    "xgboost_predictions = [1, 1, 1, 0, 0]\n",
    "random_forest_predictions = [0, 0, 1, 0, 1]\n",
    "\n",
    "# Combine predictions into a 2D array\n",
    "all_predictions = np.vstack([logistic_regression_predictions, xgboost_predictions, random_forest_predictions])\n",
    "\n",
    "# Calculate the majority vote\n",
    "ensemble_predictions = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=all_predictions)\n",
    "\n",
    "print(\"Ensemble Predictions:\", ensemble_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4285fd9c-562e-4c0c-8c3d-3d68c20a6113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Probabilities: [0.4        0.49666667 0.68333333 0.30333333 0.58666667]\n"
     ]
    }
   ],
   "source": [
    "lr_probabilities = [0.2, 0.8, 0.9, 0.4, 0.75]\n",
    "xgboost_probabilities = [0.6, 0.53, 0.6, 0.21, 0.11]\n",
    "random_forest_probabilities = [0.4, 0.16, 0.55, 0.3, 0.9]\n",
    "\n",
    "# Combine probabilities into a 2D array\n",
    "all_probabilities = np.vstack([lr_probabilities, xgboost_probabilities, random_forest_probabilities])\n",
    "\n",
    "# Calculate the average probability for each class across all models\n",
    "ensemble_probabilities = np.mean(all_probabilities, axis=0)\n",
    "\n",
    "print(\"Ensemble Probabilities:\", ensemble_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4feb8121-6513-480c-b6e4-eda41521c80a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
